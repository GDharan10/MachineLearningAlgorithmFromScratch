{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkkHcWEOq4Oxw6V33Blzza",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GDharan10/MachineLearningAlgorithmFromScratch/blob/main/Tree-Based%20Algorithms%20/%20DecisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Overview**\n",
        "\n",
        "**Used for predicting both continuous and categorical values.**"
      ],
      "metadata": {
        "id": "KxygTry9zyI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are versatile supervised learning algorithms used for both classification and regression tasks. They partition the data into subsets based on the features and recursively split the data into smaller subsets until the data in each subset is homogeneous with respect to the target variable. The goal is to create a tree that predicts the target variable by learning simple decision rules inferred from the data features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mHKNm0Erpvic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How It Works**"
      ],
      "metadata": {
        "id": "AcpejPJ5z2Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees work by recursively partitioning the data based on the feature that best separates the data at each node. Each internal node represents a \"decision\" based on a feature, and each leaf node represents the target variable's value. The tree is built using algorithms that select the best feature to split the data at each step, typically based on metrics like information gain or Gini impurity."
      ],
      "metadata": {
        "id": "WdU2MNLlp6gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Background Python Code**\n",
        "Below is the Python code for implementing linear regression from scratch."
      ],
      "metadata": {
        "id": "lHl3YVxorDwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)"
      ],
      "metadata": {
        "id": "U8gKHLWgp5mq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing the Algorithm with the Class**"
      ],
      "metadata": {
        "id": "njM_-7r8z-Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Node class represents nodes in the Decision Tree, where each node either has a split condition (feature, threshold) or a leaf value.\n",
        "\n",
        "The DecisionTree class implements the Decision Tree algorithm, allowing training (fitting) on data and making predictions. Hereâ€™s a breakdown of its components:\n",
        "\n",
        "1. **Node Class (Node):**\n",
        "\n",
        " * **Attributes:**\n",
        "\n",
        "    * **feature:** Index of the feature used for splitting at this node.\n",
        "    * **threshold:** Threshold value for the split.\n",
        "    * **left, right:** Child nodes (subtrees) for the left and right splits.\n",
        "    * **value:** Leaf node value if the node is a leaf.\n",
        "\n",
        " * **Methods:**\n",
        "\n",
        "    * **is_leaf_node():** Checks if the current node is a leaf node.\n",
        "\n",
        "2. **DecisionTree Class (DecisionTree):**\n",
        "\n",
        " * **Attributes:**\n",
        "\n",
        "    * **min_samples_split:** Minimum number of samples required to split an internal node.\n",
        "    * **max_depth:** Maximum depth of the tree.\n",
        "    * **n_features:** Number of features to consider when looking for the best split.\n",
        "    * **root:** Root node of the Decision Tree.\n",
        "\n",
        " * **Methods:**\n",
        "\n",
        "    * **fit(X, y):** Trains the Decision Tree using the provided features (X) and labels (y).\n",
        "    * **_grow_tree(X, y, depth):** Recursively grows the Decision Tree by finding the best split at each node.\n",
        "    * **_best_split(X, y, feat_idxs):** Finds the best feature and threshold to split the data based on information gain.\n",
        "    * **_information_gain(y, X_column, threshold):** Calculates the information gain when splitting based on a specific feature and threshold.\n",
        "    * **_split(X_column, split_thresh):** Splits the data into left and right indices based on a feature's threshold.\n",
        "    * **_entropy(y):** Calculates the entropy of the target variable y.\n",
        "    * **_most_common_label(y):** Returns the most common label in y.\n",
        "    * **predict(X):** Predicts the target variable for new data points X using the trained Decision Tree.\n",
        "    * **_traverse_tree(x, node):** Traverses the Decision Tree from the root node to make predictions for a single data point x.\n",
        "\n",
        "**Summary:**\n",
        " * **Node Class:** Represents nodes in the Decision Tree structure, either with split conditions or leaf values.\n",
        " * **DecisionTree Class:** Implements the Decision Tree algorithm including tree growing, splitting criteria (information gain), entropy calculation, and prediction traversal."
      ],
      "metadata": {
        "id": "HcBzlym7K0yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of the fit method**\n"
      ],
      "metadata": {
        "id": "Gmpm3XnaPQ2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initial Setup**\n",
        "\n",
        "Let's consider a simple example with 3 samples and 2 features:"
      ],
      "metadata": {
        "id": "2iIo9o00PTUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[2, 3],\n",
        "              [5, 4],\n",
        "              [9, 6],\n",
        "              [4, 7],\n",
        "              [8, 1]])\n",
        "y = np.array([0, 1, 1, 0, 1])"
      ],
      "metadata": {
        "id": "_3-rum19PbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation and Visualization**"
      ],
      "metadata": {
        "id": "zU-GgIowRGQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9msowcNLpp81",
        "outputId": "d46c7318-97dd-412a-f9ad-2d9964246250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9298245614035088\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1234\n",
        ")\n",
        "\n",
        "clf = DecisionTree(max_depth=10)\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)\n",
        "\n",
        "acc = accuracy(y_test, predictions)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation Details**"
      ],
      "metadata": {
        "id": "HHpMmlSS0OUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Considerations for DecisionTrees Modeling:**"
      ],
      "metadata": {
        "id": "C01wAPCb0ZMX"
      }
    }
  ]
}